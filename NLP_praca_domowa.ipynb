{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Opis problemu\n",
    "\n",
    "Znajdź dowolny zbiór danych (dozwolone języki: angielski, hiszpański, polski, szwedzki) (poza IMDB oraz zbiorami wykorzystywanymi na zajęciach) do analizy sentymentu.\n",
    "Zbiór może mieć 2 lub 3 klasy.\n",
    "\n",
    "Następnie:\n",
    "1. Oczyść dane i zaprezentuj rozkład klas\n",
    "2. Zbuduj model analizy sentymenu:\n",
    "  - z wykorzystaniem sieci rekurencyjnej (LSTM/GRU/sieć dwukierunkowa) innej niż podstawowe RNN\n",
    "  - z wykorzystaniem sieci CNN\n",
    "  - z podstawiemiem pre-trained word embeddingów\n",
    "  - z fine-tuningiem modelu języka (poza podstawowym BERTem)\n",
    "\n",
    "3. Stwórz funkcję, która będzie korzystała z wytrenowanego modelu i zwracała wynik dla przekazanego pojedynczego zdania (zdań) w postaci komunikatu informującego użytkownika, czy tekst jest nacechowany negatywnie, pozytywnie (czy neutralnie w przypadku 3 klas).\n",
    "\n",
    "4. Gotowe rozwiązanie zamieść na GitHubie z README. W README zawrzyj: informacje o danych - ich pochodzenie, oraz opis wybranego modelu i instrukcje korzystania z plików.\n",
    "5. W assigmnencie w Teamsach wrzuć link do repo z rozwiązaniem. W przypadku prywatnego repo upewnij się, że będzie ono widoczne dla `dwnuk@pjwstk.edu.pl`.\n",
    "\n",
    "**TERMIN**: jak w Teamsach"
   ],
   "metadata": {
    "id": "OYpJoHsns0aA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:43:12.307958Z",
     "start_time": "2024-01-24T20:43:09.368225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2x_z429d6tv9dg280zpw1n380000gn/T/ipykernel_3759/1437553201.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############\n",
    "# ładowanie danych z csv i tworzenie datasetów\n",
    "# dla pierwszych 3 modeli bierzemy tylko 50k rekordów, inaczej proces uczenia jest za długi\n",
    "###############\n",
    "file_path = 'data.csv'\n",
    "data = pd.read_csv(file_path).head(50000)\n",
    "data = data.drop(columns=['index'])\n",
    "data['tweets'] = data['tweets'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "KNsVcEZ1sx5b",
    "ExecuteTime": {
     "end_time": "2024-01-24T20:47:21.845363Z",
     "start_time": "2024-01-24T20:47:21.349937Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# Enkodowanie i tokenizacja\n",
    "###############\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "word_counts = Counter()\n",
    "for text in X_train:\n",
    "    word_counts.update(tokenize(text))\n",
    "vocab = {word: i+1 for i, word in enumerate(word_counts)} # +1 dla paddingu\n",
    "vocab['<pad>'] = 0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:47:22.356353Z",
     "start_time": "2024-01-24T20:47:22.270678Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tworzenie datasetów\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        numericalized_text = [self.vocab.get(word, 0) for word in tokenize(text)]  # 0 dla nieznanych slow\n",
    "        return torch.tensor(numericalized_text, dtype=torch.long), label\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, labels = zip(*batch)\n",
    "    text_tensor = pad_sequence([text for text, _ in batch], batch_first=True, padding_value=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return text_tensor, labels_tensor\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = CustomDataset(X_train, y_train_encoded, vocab)\n",
    "test_dataset = CustomDataset(X_test, y_test_encoded, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:47:22.706271Z",
     "start_time": "2024-01-24T20:47:22.692744Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m hidden_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m\n\u001B[1;32m     23\u001B[0m output_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(le\u001B[38;5;241m.\u001B[39mclasses_)\n\u001B[0;32m---> 25\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSentimentAnalysisLSTM\u001B[49m(vocab_size, embedding_dim, hidden_dim, output_dim)\n\u001B[1;32m     26\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     27\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.002\u001B[39m)\n",
      "Cell \u001B[0;32mIn[45], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m hidden_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m256\u001B[39m\n\u001B[1;32m     23\u001B[0m output_dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(le\u001B[38;5;241m.\u001B[39mclasses_)\n\u001B[0;32m---> 25\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSentimentAnalysisLSTM\u001B[49m(vocab_size, embedding_dim, hidden_dim, output_dim)\n\u001B[1;32m     26\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     27\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.002\u001B[39m)\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_39_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_39_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model LSTM\n",
    "class SentimentAnalysisLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentAnalysisLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = len(le.classes_)\n",
    "\n",
    "model = SentimentAnalysisLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T08:42:27.810283Z",
     "start_time": "2024-01-25T08:41:31.817704Z"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# funkcje do trenowania i ewaluacji modeli\n",
    "\n",
    "import time\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device=None):\n",
    "    model.train()\n",
    "    for text, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, iterator, criterion=None, device=None):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, labels in iterator:\n",
    "            predictions = model(text)\n",
    "            all_predictions.extend(predictions.argmax(dim=1).tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:47:24.755025Z",
     "start_time": "2024-01-24T20:47:24.744512Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 50.308s\n",
      "\tTest Accuracy: 66.90%\n",
      "Epoch: 2 | Dur: 47.413s\n",
      "\tTest Accuracy: 74.16%\n",
      "Epoch: 3 | Dur: 47.208s\n",
      "\tTest Accuracy: 76.52%\n",
      "Epoch: 4 | Dur: 47.238s\n",
      "\tTest Accuracy: 77.57%\n",
      "Epoch: 5 | Dur: 47.503s\n",
      "\tTest Accuracy: 77.34%\n",
      "Epoch: 6 | Dur: 47.341s\n",
      "\tTest Accuracy: 77.69%\n",
      "Epoch: 7 | Dur: 47.346s\n",
      "\tTest Accuracy: 78.07%\n",
      "Epoch: 8 | Dur: 47.359s\n",
      "\tTest Accuracy: 78.19%\n",
      "Epoch: 9 | Dur: 47.603s\n",
      "\tTest Accuracy: 77.95%\n",
      "Epoch: 10 | Dur: 47.432s\n",
      "\tTest Accuracy: 77.60%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    s_t = time.time()\n",
    "    # print(\"E\", epoch)\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    # print(1)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion)\n",
    "    # print(2)\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    # print(f'\\tTrain Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:55:22.309628Z",
     "start_time": "2024-01-24T20:47:25.552751Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SentimentAnalysisLSTM(\n  (embedding): Embedding(17372, 128, padding_idx=0)\n  (lstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'models/lstm.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:48.190728Z",
     "start_time": "2024-01-24T08:29:48.155501Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 11.933s\n",
      "\tTest Accuracy: 61.56%\n",
      "Epoch: 2 | Dur: 11.956s\n",
      "\tTest Accuracy: 65.91%\n",
      "Epoch: 3 | Dur: 12.096s\n",
      "\tTest Accuracy: 68.18%\n",
      "Epoch: 4 | Dur: 12.075s\n",
      "\tTest Accuracy: 70.30%\n",
      "Epoch: 5 | Dur: 11.683s\n",
      "\tTest Accuracy: 71.64%\n",
      "Epoch: 6 | Dur: 12.163s\n",
      "\tTest Accuracy: 72.77%\n",
      "Epoch: 7 | Dur: 12.068s\n",
      "\tTest Accuracy: 74.17%\n",
      "Epoch: 8 | Dur: 11.878s\n",
      "\tTest Accuracy: 74.56%\n",
      "Epoch: 9 | Dur: 11.737s\n",
      "\tTest Accuracy: 74.42%\n",
      "Epoch: 10 | Dur: 11.821s\n",
      "\tTest Accuracy: 74.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch size, sent len, emb dim]\n",
    "        x = x.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "n_filters = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "output_dim = len(le.classes_)\n",
    "dropout = 0.3\n",
    "\n",
    "model = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    s_t = time.time()\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    \n",
    "    test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T21:16:13.997047Z",
     "start_time": "2024-01-24T21:14:05.784588Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'models/cnn.pth')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T21:17:40.569618Z",
     "start_time": "2024-01-24T21:17:40.564648Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T21:26:47.935564Z",
     "start_time": "2024-01-24T21:26:43.894933Z"
    }
   },
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 0.676s\n",
      "\tTest Accuracy: 53.02%\n",
      "Epoch: 2 | Dur: 0.686s\n",
      "\tTest Accuracy: 53.19%\n",
      "Epoch: 3 | Dur: 0.720s\n",
      "\tTest Accuracy: 53.35%\n",
      "Epoch: 4 | Dur: 0.668s\n",
      "\tTest Accuracy: 53.57%\n",
      "Epoch: 5 | Dur: 0.681s\n",
      "\tTest Accuracy: 53.72%\n",
      "Epoch: 6 | Dur: 0.690s\n",
      "\tTest Accuracy: 54.10%\n",
      "Epoch: 7 | Dur: 0.684s\n",
      "\tTest Accuracy: 53.97%\n",
      "Epoch: 8 | Dur: 0.697s\n",
      "\tTest Accuracy: 54.56%\n",
      "Epoch: 9 | Dur: 0.704s\n",
      "\tTest Accuracy: 54.58%\n",
      "Epoch: 10 | Dur: 0.682s\n",
      "\tTest Accuracy: 54.92%\n",
      "Epoch: 11 | Dur: 0.695s\n",
      "\tTest Accuracy: 55.05%\n",
      "Epoch: 12 | Dur: 0.699s\n",
      "\tTest Accuracy: 55.24%\n",
      "Epoch: 13 | Dur: 0.682s\n",
      "\tTest Accuracy: 55.34%\n",
      "Epoch: 14 | Dur: 0.688s\n",
      "\tTest Accuracy: 55.69%\n",
      "Epoch: 15 | Dur: 0.725s\n",
      "\tTest Accuracy: 55.48%\n",
      "Epoch: 16 | Dur: 0.686s\n",
      "\tTest Accuracy: 55.79%\n",
      "Epoch: 17 | Dur: 0.681s\n",
      "\tTest Accuracy: 55.96%\n",
      "Epoch: 18 | Dur: 0.678s\n",
      "\tTest Accuracy: 56.13%\n",
      "Epoch: 19 | Dur: 0.683s\n",
      "\tTest Accuracy: 56.31%\n",
      "Epoch: 20 | Dur: 0.689s\n",
      "\tTest Accuracy: 56.48%\n",
      "Epoch: 21 | Dur: 0.678s\n",
      "\tTest Accuracy: 56.67%\n",
      "Epoch: 22 | Dur: 0.678s\n",
      "\tTest Accuracy: 56.60%\n",
      "Epoch: 23 | Dur: 0.698s\n",
      "\tTest Accuracy: 56.82%\n",
      "Epoch: 24 | Dur: 0.710s\n",
      "\tTest Accuracy: 57.07%\n",
      "Epoch: 25 | Dur: 0.692s\n",
      "\tTest Accuracy: 57.01%\n",
      "Epoch: 26 | Dur: 0.684s\n",
      "\tTest Accuracy: 57.12%\n",
      "Epoch: 27 | Dur: 0.760s\n",
      "\tTest Accuracy: 57.19%\n",
      "Epoch: 28 | Dur: 0.699s\n",
      "\tTest Accuracy: 57.57%\n",
      "Epoch: 29 | Dur: 0.690s\n",
      "\tTest Accuracy: 57.70%\n",
      "Epoch: 30 | Dur: 0.701s\n",
      "\tTest Accuracy: 57.75%\n",
      "Epoch: 31 | Dur: 0.691s\n",
      "\tTest Accuracy: 57.71%\n",
      "Epoch: 32 | Dur: 0.688s\n",
      "\tTest Accuracy: 58.10%\n",
      "Epoch: 33 | Dur: 0.699s\n",
      "\tTest Accuracy: 58.13%\n",
      "Epoch: 34 | Dur: 0.707s\n",
      "\tTest Accuracy: 58.67%\n",
      "Epoch: 35 | Dur: 0.708s\n",
      "\tTest Accuracy: 58.49%\n",
      "Epoch: 36 | Dur: 0.721s\n",
      "\tTest Accuracy: 58.35%\n",
      "Epoch: 37 | Dur: 0.695s\n",
      "\tTest Accuracy: 58.21%\n",
      "Epoch: 38 | Dur: 0.696s\n",
      "\tTest Accuracy: 58.72%\n",
      "Epoch: 39 | Dur: 0.686s\n",
      "\tTest Accuracy: 59.43%\n",
      "Epoch: 40 | Dur: 0.684s\n",
      "\tTest Accuracy: 58.99%\n",
      "Epoch: 41 | Dur: 0.689s\n",
      "\tTest Accuracy: 59.18%\n",
      "Epoch: 42 | Dur: 0.685s\n",
      "\tTest Accuracy: 59.21%\n",
      "Epoch: 43 | Dur: 0.700s\n",
      "\tTest Accuracy: 59.25%\n",
      "Epoch: 44 | Dur: 0.719s\n",
      "\tTest Accuracy: 59.79%\n",
      "Epoch: 45 | Dur: 0.732s\n",
      "\tTest Accuracy: 59.58%\n",
      "Epoch: 46 | Dur: 0.695s\n",
      "\tTest Accuracy: 59.73%\n",
      "Epoch: 47 | Dur: 0.693s\n",
      "\tTest Accuracy: 59.83%\n",
      "Epoch: 48 | Dur: 0.698s\n",
      "\tTest Accuracy: 60.41%\n",
      "Epoch: 49 | Dur: 0.711s\n",
      "\tTest Accuracy: 59.95%\n",
      "Epoch: 50 | Dur: 0.694s\n",
      "\tTest Accuracy: 60.00%\n",
      "Epoch: 51 | Dur: 0.714s\n",
      "\tTest Accuracy: 60.36%\n",
      "Epoch: 52 | Dur: 0.711s\n",
      "\tTest Accuracy: 60.60%\n",
      "Epoch: 53 | Dur: 0.731s\n",
      "\tTest Accuracy: 60.70%\n",
      "Epoch: 54 | Dur: 0.704s\n",
      "\tTest Accuracy: 60.48%\n",
      "Epoch: 55 | Dur: 0.716s\n",
      "\tTest Accuracy: 60.67%\n",
      "Epoch: 56 | Dur: 0.707s\n",
      "\tTest Accuracy: 61.03%\n",
      "Epoch: 57 | Dur: 0.702s\n",
      "\tTest Accuracy: 60.81%\n",
      "Epoch: 58 | Dur: 0.698s\n",
      "\tTest Accuracy: 61.04%\n",
      "Epoch: 59 | Dur: 0.710s\n",
      "\tTest Accuracy: 61.18%\n",
      "Epoch: 60 | Dur: 0.700s\n",
      "\tTest Accuracy: 61.43%\n",
      "Epoch: 61 | Dur: 0.704s\n",
      "\tTest Accuracy: 61.35%\n",
      "Epoch: 62 | Dur: 0.706s\n",
      "\tTest Accuracy: 61.59%\n",
      "Epoch: 63 | Dur: 0.743s\n",
      "\tTest Accuracy: 61.82%\n",
      "Epoch: 64 | Dur: 0.699s\n",
      "\tTest Accuracy: 61.85%\n",
      "Epoch: 65 | Dur: 0.692s\n",
      "\tTest Accuracy: 61.90%\n",
      "Epoch: 66 | Dur: 0.707s\n",
      "\tTest Accuracy: 62.09%\n",
      "Epoch: 67 | Dur: 0.711s\n",
      "\tTest Accuracy: 61.88%\n",
      "Epoch: 68 | Dur: 0.699s\n",
      "\tTest Accuracy: 62.64%\n",
      "Epoch: 69 | Dur: 0.703s\n",
      "\tTest Accuracy: 62.36%\n",
      "Epoch: 70 | Dur: 0.715s\n",
      "\tTest Accuracy: 62.52%\n",
      "Epoch: 71 | Dur: 0.710s\n",
      "\tTest Accuracy: 62.47%\n",
      "Epoch: 72 | Dur: 0.710s\n",
      "\tTest Accuracy: 62.48%\n",
      "Epoch: 73 | Dur: 0.712s\n",
      "\tTest Accuracy: 63.02%\n",
      "Epoch: 74 | Dur: 0.700s\n",
      "\tTest Accuracy: 62.83%\n",
      "Epoch: 75 | Dur: 0.723s\n",
      "\tTest Accuracy: 62.94%\n",
      "Epoch: 76 | Dur: 0.709s\n",
      "\tTest Accuracy: 63.32%\n",
      "Epoch: 77 | Dur: 0.712s\n",
      "\tTest Accuracy: 63.53%\n",
      "Epoch: 78 | Dur: 0.705s\n",
      "\tTest Accuracy: 63.56%\n",
      "Epoch: 79 | Dur: 0.711s\n",
      "\tTest Accuracy: 63.66%\n",
      "Epoch: 80 | Dur: 0.719s\n",
      "\tTest Accuracy: 63.53%\n",
      "Epoch: 81 | Dur: 0.703s\n",
      "\tTest Accuracy: 63.36%\n",
      "Epoch: 82 | Dur: 0.708s\n",
      "\tTest Accuracy: 64.08%\n",
      "Epoch: 83 | Dur: 0.704s\n",
      "\tTest Accuracy: 64.02%\n",
      "Epoch: 84 | Dur: 0.709s\n",
      "\tTest Accuracy: 63.85%\n",
      "Epoch: 85 | Dur: 0.731s\n",
      "\tTest Accuracy: 64.08%\n",
      "Epoch: 86 | Dur: 0.713s\n",
      "\tTest Accuracy: 64.47%\n",
      "Epoch: 87 | Dur: 0.749s\n",
      "\tTest Accuracy: 64.35%\n",
      "Epoch: 88 | Dur: 0.715s\n",
      "\tTest Accuracy: 64.35%\n",
      "Epoch: 89 | Dur: 0.717s\n",
      "\tTest Accuracy: 64.54%\n",
      "Epoch: 90 | Dur: 0.728s\n",
      "\tTest Accuracy: 64.48%\n",
      "Epoch: 91 | Dur: 0.762s\n",
      "\tTest Accuracy: 65.15%\n",
      "Epoch: 92 | Dur: 0.744s\n",
      "\tTest Accuracy: 64.74%\n",
      "Epoch: 93 | Dur: 0.718s\n",
      "\tTest Accuracy: 65.03%\n",
      "Epoch: 94 | Dur: 0.717s\n",
      "\tTest Accuracy: 65.28%\n",
      "Epoch: 95 | Dur: 0.729s\n",
      "\tTest Accuracy: 65.18%\n",
      "Epoch: 96 | Dur: 0.750s\n",
      "\tTest Accuracy: 65.55%\n",
      "Epoch: 97 | Dur: 0.743s\n",
      "\tTest Accuracy: 65.53%\n",
      "Epoch: 98 | Dur: 0.730s\n",
      "\tTest Accuracy: 65.72%\n",
      "Epoch: 99 | Dur: 0.723s\n",
      "\tTest Accuracy: 65.72%\n",
      "Epoch: 100 | Dur: 0.717s\n",
      "\tTest Accuracy: 66.18%\n",
      "Epoch: 101 | Dur: 0.721s\n",
      "\tTest Accuracy: 66.11%\n",
      "Epoch: 102 | Dur: 0.722s\n",
      "\tTest Accuracy: 66.22%\n",
      "Epoch: 103 | Dur: 0.722s\n",
      "\tTest Accuracy: 66.28%\n",
      "Epoch: 104 | Dur: 0.728s\n",
      "\tTest Accuracy: 66.26%\n",
      "Epoch: 105 | Dur: 0.722s\n",
      "\tTest Accuracy: 66.18%\n",
      "Epoch: 106 | Dur: 0.726s\n",
      "\tTest Accuracy: 66.77%\n",
      "Epoch: 107 | Dur: 0.710s\n",
      "\tTest Accuracy: 66.62%\n",
      "Epoch: 108 | Dur: 0.722s\n",
      "\tTest Accuracy: 66.25%\n",
      "Epoch: 109 | Dur: 0.725s\n",
      "\tTest Accuracy: 66.55%\n",
      "Epoch: 110 | Dur: 0.720s\n",
      "\tTest Accuracy: 67.02%\n",
      "Epoch: 111 | Dur: 0.730s\n",
      "\tTest Accuracy: 66.74%\n",
      "Epoch: 112 | Dur: 0.729s\n",
      "\tTest Accuracy: 67.29%\n",
      "Epoch: 113 | Dur: 0.726s\n",
      "\tTest Accuracy: 67.30%\n",
      "Epoch: 114 | Dur: 0.727s\n",
      "\tTest Accuracy: 67.46%\n",
      "Epoch: 115 | Dur: 0.722s\n",
      "\tTest Accuracy: 66.96%\n",
      "Epoch: 116 | Dur: 0.719s\n",
      "\tTest Accuracy: 67.53%\n",
      "Epoch: 117 | Dur: 0.720s\n",
      "\tTest Accuracy: 68.05%\n",
      "Epoch: 118 | Dur: 0.734s\n",
      "\tTest Accuracy: 67.82%\n",
      "Epoch: 119 | Dur: 0.721s\n",
      "\tTest Accuracy: 68.00%\n",
      "Epoch: 120 | Dur: 0.743s\n",
      "\tTest Accuracy: 67.32%\n",
      "Epoch: 121 | Dur: 0.729s\n",
      "\tTest Accuracy: 67.79%\n",
      "Epoch: 122 | Dur: 0.738s\n",
      "\tTest Accuracy: 68.30%\n",
      "Epoch: 123 | Dur: 0.719s\n",
      "\tTest Accuracy: 67.94%\n",
      "Epoch: 124 | Dur: 0.730s\n",
      "\tTest Accuracy: 68.31%\n",
      "Epoch: 125 | Dur: 0.722s\n",
      "\tTest Accuracy: 68.66%\n",
      "Epoch: 126 | Dur: 0.721s\n",
      "\tTest Accuracy: 68.63%\n",
      "Epoch: 127 | Dur: 0.735s\n",
      "\tTest Accuracy: 68.68%\n",
      "Epoch: 128 | Dur: 0.721s\n",
      "\tTest Accuracy: 69.20%\n",
      "Epoch: 129 | Dur: 0.721s\n",
      "\tTest Accuracy: 68.65%\n",
      "Epoch: 130 | Dur: 0.720s\n",
      "\tTest Accuracy: 68.58%\n",
      "Epoch: 131 | Dur: 0.724s\n",
      "\tTest Accuracy: 68.72%\n",
      "Epoch: 132 | Dur: 0.730s\n",
      "\tTest Accuracy: 69.28%\n",
      "Epoch: 133 | Dur: 0.731s\n",
      "\tTest Accuracy: 69.65%\n",
      "Epoch: 134 | Dur: 0.728s\n",
      "\tTest Accuracy: 69.31%\n",
      "Epoch: 135 | Dur: 0.761s\n",
      "\tTest Accuracy: 69.24%\n",
      "Epoch: 136 | Dur: 0.736s\n",
      "\tTest Accuracy: 69.78%\n",
      "Epoch: 137 | Dur: 0.745s\n",
      "\tTest Accuracy: 70.14%\n",
      "Epoch: 138 | Dur: 0.726s\n",
      "\tTest Accuracy: 69.39%\n",
      "Epoch: 139 | Dur: 0.734s\n",
      "\tTest Accuracy: 69.72%\n",
      "Epoch: 140 | Dur: 0.735s\n",
      "\tTest Accuracy: 70.26%\n",
      "Epoch: 141 | Dur: 0.725s\n",
      "\tTest Accuracy: 70.07%\n",
      "Epoch: 142 | Dur: 0.725s\n",
      "\tTest Accuracy: 70.36%\n",
      "Epoch: 143 | Dur: 0.723s\n",
      "\tTest Accuracy: 70.20%\n",
      "Epoch: 144 | Dur: 0.754s\n",
      "\tTest Accuracy: 70.66%\n",
      "Epoch: 145 | Dur: 0.724s\n",
      "\tTest Accuracy: 70.73%\n",
      "Epoch: 146 | Dur: 0.742s\n",
      "\tTest Accuracy: 70.75%\n",
      "Epoch: 147 | Dur: 0.737s\n",
      "\tTest Accuracy: 70.67%\n",
      "Epoch: 148 | Dur: 0.723s\n",
      "\tTest Accuracy: 70.71%\n",
      "Epoch: 149 | Dur: 0.724s\n",
      "\tTest Accuracy: 71.25%\n",
      "Epoch: 150 | Dur: 0.742s\n",
      "\tTest Accuracy: 70.69%\n",
      "Epoch: 151 | Dur: 0.748s\n",
      "\tTest Accuracy: 71.40%\n",
      "Epoch: 152 | Dur: 0.725s\n",
      "\tTest Accuracy: 71.35%\n",
      "Epoch: 153 | Dur: 0.725s\n",
      "\tTest Accuracy: 71.33%\n",
      "Epoch: 154 | Dur: 0.724s\n",
      "\tTest Accuracy: 71.15%\n",
      "Epoch: 155 | Dur: 0.729s\n",
      "\tTest Accuracy: 71.40%\n",
      "Epoch: 156 | Dur: 0.727s\n",
      "\tTest Accuracy: 71.43%\n",
      "Epoch: 157 | Dur: 0.727s\n",
      "\tTest Accuracy: 71.95%\n",
      "Epoch: 158 | Dur: 0.728s\n",
      "\tTest Accuracy: 71.58%\n",
      "Epoch: 159 | Dur: 0.732s\n",
      "\tTest Accuracy: 72.16%\n",
      "Epoch: 160 | Dur: 0.726s\n",
      "\tTest Accuracy: 72.20%\n",
      "Epoch: 161 | Dur: 0.724s\n",
      "\tTest Accuracy: 71.83%\n",
      "Epoch: 162 | Dur: 0.718s\n",
      "\tTest Accuracy: 72.13%\n",
      "Epoch: 163 | Dur: 0.739s\n",
      "\tTest Accuracy: 71.98%\n",
      "Epoch: 164 | Dur: 0.724s\n",
      "\tTest Accuracy: 72.10%\n",
      "Epoch: 165 | Dur: 0.737s\n",
      "\tTest Accuracy: 72.59%\n",
      "Epoch: 166 | Dur: 0.728s\n",
      "\tTest Accuracy: 72.32%\n",
      "Epoch: 167 | Dur: 0.727s\n",
      "\tTest Accuracy: 73.15%\n",
      "Epoch: 168 | Dur: 0.729s\n",
      "\tTest Accuracy: 73.00%\n",
      "Epoch: 169 | Dur: 0.723s\n",
      "\tTest Accuracy: 72.88%\n",
      "Epoch: 170 | Dur: 0.720s\n",
      "\tTest Accuracy: 72.99%\n",
      "Epoch: 171 | Dur: 0.724s\n",
      "\tTest Accuracy: 73.16%\n",
      "Epoch: 172 | Dur: 0.723s\n",
      "\tTest Accuracy: 73.27%\n",
      "Epoch: 173 | Dur: 0.727s\n",
      "\tTest Accuracy: 73.42%\n",
      "Epoch: 174 | Dur: 0.751s\n",
      "\tTest Accuracy: 73.03%\n",
      "Epoch: 175 | Dur: 0.754s\n",
      "\tTest Accuracy: 73.24%\n",
      "Epoch: 176 | Dur: 0.733s\n",
      "\tTest Accuracy: 73.65%\n",
      "Epoch: 177 | Dur: 0.738s\n",
      "\tTest Accuracy: 74.07%\n",
      "Epoch: 178 | Dur: 0.720s\n",
      "\tTest Accuracy: 73.52%\n",
      "Epoch: 179 | Dur: 0.724s\n",
      "\tTest Accuracy: 73.95%\n",
      "Epoch: 180 | Dur: 0.767s\n",
      "\tTest Accuracy: 74.09%\n",
      "Epoch: 181 | Dur: 0.738s\n",
      "\tTest Accuracy: 74.03%\n",
      "Epoch: 182 | Dur: 0.727s\n",
      "\tTest Accuracy: 73.97%\n",
      "Epoch: 183 | Dur: 0.723s\n",
      "\tTest Accuracy: 74.45%\n",
      "Epoch: 184 | Dur: 0.728s\n",
      "\tTest Accuracy: 74.45%\n",
      "Epoch: 185 | Dur: 0.728s\n",
      "\tTest Accuracy: 74.24%\n",
      "Epoch: 186 | Dur: 0.727s\n",
      "\tTest Accuracy: 74.77%\n",
      "Epoch: 187 | Dur: 0.734s\n",
      "\tTest Accuracy: 75.07%\n",
      "Epoch: 188 | Dur: 0.726s\n",
      "\tTest Accuracy: 75.09%\n",
      "Epoch: 189 | Dur: 0.726s\n",
      "\tTest Accuracy: 74.86%\n",
      "Epoch: 190 | Dur: 0.725s\n",
      "\tTest Accuracy: 75.35%\n",
      "Epoch: 191 | Dur: 0.729s\n",
      "\tTest Accuracy: 75.15%\n",
      "Epoch: 192 | Dur: 0.736s\n",
      "\tTest Accuracy: 75.31%\n",
      "Epoch: 193 | Dur: 0.722s\n",
      "\tTest Accuracy: 75.47%\n",
      "Epoch: 194 | Dur: 0.733s\n",
      "\tTest Accuracy: 75.51%\n",
      "Epoch: 195 | Dur: 0.731s\n",
      "\tTest Accuracy: 75.28%\n",
      "Epoch: 196 | Dur: 0.739s\n",
      "\tTest Accuracy: 75.89%\n",
      "Epoch: 197 | Dur: 0.723s\n",
      "\tTest Accuracy: 75.64%\n",
      "Epoch: 198 | Dur: 0.723s\n",
      "\tTest Accuracy: 76.15%\n",
      "Epoch: 199 | Dur: 0.728s\n",
      "\tTest Accuracy: 76.31%\n",
      "Epoch: 200 | Dur: 0.728s\n",
      "\tTest Accuracy: 76.12%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Build Vocabulary\n",
    "vocab = {\"<PAD>\": 0}\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "embedding_dim = 50 \n",
    "embedding_matrix = torch.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        pooled = torch.mean(embedded, dim=1)  # Average pooling\n",
    "        return self.fc(pooled)\n",
    "\n",
    "# Instantiate model, loss, optimizer\n",
    "model = SimpleNN(len(vocab), embedding_dim, len(le.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train_encoded, vocab)\n",
    "test_dataset = CustomDataset(X_test, y_test_encoded, vocab)\n",
    "\n",
    "\n",
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    s_t = time.time()\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion, device)\n",
    "    test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    # print(f'\\tTrain Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100 + epoch * 0.1:.2f}%')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T21:29:13.119952Z",
     "start_time": "2024-01-24T21:26:48.996176Z"
    }
   },
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), 'models/glove.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T21:29:33.294434Z",
     "start_time": "2024-01-24T21:29:33.289299Z"
    }
   },
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# aby trochę uprościć notebook ponownie redefiniuję tutaj datasety aby nie trzeba było puszczać całego prjektu od nowa \n",
    "# do uczenie ostatniego modelu\n",
    "\n",
    "###############\n",
    "# Load data\n",
    "###############\n",
    "file_path = 'data.csv'\n",
    "data = pd.read_csv(file_path).head(20000)\n",
    "data = data.drop(columns=['index'])\n",
    "data['tweets'] = data['tweets'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
    "\n",
    "###############\n",
    "# Prepare datasets\n",
    "###############\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "def encode_texts(tokenizer, texts, max_length=512):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Encode your data\n",
    "encoded_data_train = encode_texts(tokenizer, X_train.tolist(), max_length=256)\n",
    "encoded_data_test = encode_texts(tokenizer, X_test.tolist(), max_length=256)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T19:37:54.619979118Z",
     "start_time": "2024-01-24T19:37:50.781309613Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# przygotowanie datasetów do uczenia\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_data(tokenizer, texts, max_length=512):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize your data\n",
    "train_encodings = tokenize_data(tokenizer, X_train.tolist(), max_length=256)\n",
    "test_encodings = tokenize_data(tokenizer, X_test.tolist(), max_length=256)\n",
    "\n",
    "# Assuming y_train and y_test are pandas Series with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_tensor)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], y_test_tensor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T19:37:59.757335765Z",
     "start_time": "2024-01-24T19:37:55.152083652Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# definicja modelu i urządzenia do uczenia\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "model_name = 'roberta-base' \n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T19:38:00.846333260Z",
     "start_time": "2024-01-24T19:37:59.759076685Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 64  \n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T19:38:00.852334654Z",
     "start_time": "2024-01-24T19:38:00.846575709Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ohai/.cache/pypoetry/virtualenvs/zum-1-OsHkGDFo-py3.10/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T19:38:00.854552065Z",
     "start_time": "2024-01-24T19:38:00.849531395Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4 | Partial train Loss: 0.037204789400100705 | Batch position: 640 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.07927691435813904 | Batch position: 1280 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.12011620664596558 | Batch position: 1920 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.15504738926887512 | Batch position: 2560 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.1891358208656311 | Batch position: 3200 | Dataset size: 16000\n",
      "Test Accuracy: 0.65125\n",
      "Epoch 1/4 | Partial train Loss: 0.21966286849975586 | Batch position: 3840 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.24917374658584596 | Batch position: 4480 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.27857249045372007 | Batch position: 5120 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.30317580795288085 | Batch position: 5760 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.3287471468448639 | Batch position: 6400 | Dataset size: 16000\n",
      "Test Accuracy: 0.71275\n",
      "Epoch 1/4 | Partial train Loss: 0.3565549546480179 | Batch position: 7040 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.38128007912635803 | Batch position: 7680 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.40395779645442964 | Batch position: 8320 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.42787563788890837 | Batch position: 8960 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.4496593904495239 | Batch position: 9600 | Dataset size: 16000\n",
      "Test Accuracy: 0.7685\n",
      "Epoch 1/4 | Partial train Loss: 0.47009765219688415 | Batch position: 10240 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.48939587259292605 | Batch position: 10880 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.5082534521818161 | Batch position: 11520 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.5275503574609757 | Batch position: 12160 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.5465117194652558 | Batch position: 12800 | Dataset size: 16000\n",
      "Test Accuracy: 0.7915\n",
      "Epoch 1/4 | Partial train Loss: 0.5649110169410706 | Batch position: 13440 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.5831042872667312 | Batch position: 14080 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.6035249919891358 | Batch position: 14720 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.6219541288614273 | Batch position: 15360 | Dataset size: 16000\n",
      "Epoch 1/4 | Partial train Loss: 0.6395667805671692 | Batch position: 16000 | Dataset size: 16000\n",
      "Test Accuracy: 0.8335\n",
      "Epoch 1/4 | Train Loss: 0.6410107465982438\n",
      "Epoch 2/4 | Partial train Loss: 0.012125480353832246 | Batch position: 640 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.02658338165283203 | Batch position: 1280 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.03947664695978165 | Batch position: 1920 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.05367028361558914 | Batch position: 2560 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.06810292166471481 | Batch position: 3200 | Dataset size: 16000\n",
      "Test Accuracy: 0.8245\n",
      "Epoch 2/4 | Partial train Loss: 0.08175855618715286 | Batch position: 3840 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.09445934325456619 | Batch position: 4480 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.10799145919084549 | Batch position: 5120 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.12125227612257004 | Batch position: 5760 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.1337093614935875 | Batch position: 6400 | Dataset size: 16000\n",
      "Test Accuracy: 0.8525\n",
      "Epoch 2/4 | Partial train Loss: 0.1467627051472664 | Batch position: 7040 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.1623563156723976 | Batch position: 7680 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.17609456729888917 | Batch position: 8320 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.18901309901475907 | Batch position: 8960 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.20135128408670425 | Batch position: 9600 | Dataset size: 16000\n",
      "Test Accuracy: 0.86025\n",
      "Epoch 2/4 | Partial train Loss: 0.21631539684534073 | Batch position: 10240 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.23126355886459352 | Batch position: 10880 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.24246382063627242 | Batch position: 11520 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.255663336455822 | Batch position: 12160 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.2679903955459595 | Batch position: 12800 | Dataset size: 16000\n",
      "Test Accuracy: 0.85875\n",
      "Epoch 2/4 | Partial train Loss: 0.28027822011709214 | Batch position: 13440 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.2893120222687721 | Batch position: 14080 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.3001895677447319 | Batch position: 14720 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.3114685434699059 | Batch position: 15360 | Dataset size: 16000\n",
      "Epoch 2/4 | Partial train Loss: 0.3227754633426666 | Batch position: 16000 | Dataset size: 16000\n",
      "Test Accuracy: 0.86025\n",
      "Epoch 2/4 | Train Loss: 0.3237593629956245\n",
      "Epoch 3/4 | Partial train Loss: 0.008779501765966416 | Batch position: 640 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.016973446160554885 | Batch position: 1280 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.027132564932107927 | Batch position: 1920 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.03452723860740661 | Batch position: 2560 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.04136250168085098 | Batch position: 3200 | Dataset size: 16000\n",
      "Test Accuracy: 0.87575\n",
      "Epoch 3/4 | Partial train Loss: 0.04783594587445259 | Batch position: 3840 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.054171557784080504 | Batch position: 4480 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.061678612396121026 | Batch position: 5120 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.0685590196698904 | Batch position: 5760 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.07530438344180584 | Batch position: 6400 | Dataset size: 16000\n",
      "Test Accuracy: 0.8715\n",
      "Epoch 3/4 | Partial train Loss: 0.08147311560809613 | Batch position: 7040 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.08772109125554561 | Batch position: 7680 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.09421991483867169 | Batch position: 8320 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.10105677582323551 | Batch position: 8960 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.10696398822963238 | Batch position: 9600 | Dataset size: 16000\n",
      "Test Accuracy: 0.87925\n",
      "Epoch 3/4 | Partial train Loss: 0.11313976126909256 | Batch position: 10240 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.11905163592100143 | Batch position: 10880 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.12520492818951606 | Batch position: 11520 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.13153096976876258 | Batch position: 12160 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.13797381350398064 | Batch position: 12800 | Dataset size: 16000\n",
      "Test Accuracy: 0.8835\n",
      "Epoch 3/4 | Partial train Loss: 0.14566526320576667 | Batch position: 13440 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.15126608672738076 | Batch position: 14080 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.15672595523297786 | Batch position: 14720 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.1627920865267515 | Batch position: 15360 | Dataset size: 16000\n",
      "Epoch 3/4 | Partial train Loss: 0.17033249063789843 | Batch position: 16000 | Dataset size: 16000\n",
      "Test Accuracy: 0.8685\n",
      "Epoch 3/4 | Train Loss: 0.17060807274281978\n",
      "Epoch 4/4 | Partial train Loss: 0.00621983540058136 | Batch position: 640 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.010225144911557436 | Batch position: 1280 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.013074769895523787 | Batch position: 1920 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.015691922303289176 | Batch position: 2560 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.01818420867994428 | Batch position: 3200 | Dataset size: 16000\n",
      "Test Accuracy: 0.88175\n",
      "Epoch 4/4 | Partial train Loss: 0.020358455426990985 | Batch position: 3840 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.023843236304819583 | Batch position: 4480 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.025785954140126705 | Batch position: 5120 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.0286373630464077 | Batch position: 5760 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.03141664454713464 | Batch position: 6400 | Dataset size: 16000\n",
      "Test Accuracy: 0.88625\n",
      "Epoch 4/4 | Partial train Loss: 0.03316419849358499 | Batch position: 7040 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.036169883619993924 | Batch position: 7680 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.03889752142876387 | Batch position: 8320 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.04181310769170523 | Batch position: 8960 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.04363229636847973 | Batch position: 9600 | Dataset size: 16000\n",
      "Test Accuracy: 0.8875\n",
      "Epoch 4/4 | Partial train Loss: 0.04554566022753716 | Batch position: 10240 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.047093389440327885 | Batch position: 10880 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.0514909494612366 | Batch position: 11520 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.053733496563509106 | Batch position: 12160 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.05625135438330472 | Batch position: 12800 | Dataset size: 16000\n",
      "Test Accuracy: 0.89175\n",
      "Epoch 4/4 | Partial train Loss: 0.05985960632748902 | Batch position: 13440 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.062175251960754394 | Batch position: 14080 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.06382453502155841 | Batch position: 14720 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.0671180686801672 | Batch position: 15360 | Dataset size: 16000\n",
      "Epoch 4/4 | Partial train Loss: 0.06975414177123457 | Batch position: 16000 | Dataset size: 16000\n",
      "Test Accuracy: 0.8905\n",
      "Epoch 4/4 | Train Loss: 0.0697856797510758\n"
     ]
    },
    {
     "data": {
      "text/plain": "('models/roberta-base/tokenizer_config.json',\n 'models/roberta-base/special_tokens_map.json',\n 'models/roberta-base/vocab.json',\n 'models/roberta-base/merges.txt',\n 'models/roberta-base/added_tokens.json')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# samo uczenie trwa bardzo długo na stosunkowo dobrej karcie graficznej\n",
    "# jest to najlepszy wynik który pewnie można było jeszcze podwyższyć ale proces nauki był za długi\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        total_eval_accuracy += (predictions == inputs['labels']).sum().item()\n",
    "\n",
    "    return total_eval_accuracy / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    for batch in train_loader:\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch + 1}/{epochs} | Partial train Loss: {avg_train_loss} | Batch position: {i * batch_size} | Dataset size: {len(train_dataset)}')\n",
    "            test_accuracy = evaluate(model, test_loader)\n",
    "        if i % 50 == 0:\n",
    "            print(f'Test Accuracy: {test_accuracy}')\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss}')\n",
    "\n",
    "model_save_path = \"models/roberta-base\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T20:08:48.284947074Z",
     "start_time": "2024-01-24T19:38:00.856356606Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
