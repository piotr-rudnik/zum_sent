{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Opis problemu\n",
    "\n",
    "Znajdź dowolny zbiór danych (dozwolone języki: angielski, hiszpański, polski, szwedzki) (poza IMDB oraz zbiorami wykorzystywanymi na zajęciach) do analizy sentymentu.\n",
    "Zbiór może mieć 2 lub 3 klasy.\n",
    "\n",
    "Następnie:\n",
    "1. Oczyść dane i zaprezentuj rozkład klas\n",
    "2. Zbuduj model analizy sentymenu:\n",
    "  - z wykorzystaniem sieci rekurencyjnej (LSTM/GRU/sieć dwukierunkowa) innej niż podstawowe RNN\n",
    "  - z wykorzystaniem sieci CNN\n",
    "  - z podstawiemiem pre-trained word embeddingów\n",
    "  - z fine-tuningiem modelu języka (poza podstawowym BERTem)\n",
    "\n",
    "3. Stwórz funkcję, która będzie korzystała z wytrenowanego modelu i zwracała wynik dla przekazanego pojedynczego zdania (zdań) w postaci komunikatu informującego użytkownika, czy tekst jest nacechowany negatywnie, pozytywnie (czy neutralnie w przypadku 3 klas).\n",
    "\n",
    "4. Gotowe rozwiązanie zamieść na GitHubie z README. W README zawrzyj: informacje o danych - ich pochodzenie, oraz opis wybranego modelu i instrukcje korzystania z plików.\n",
    "5. W assigmnencie w Teamsach wrzuć link do repo z rozwiązaniem. W przypadku prywatnego repo upewnij się, że będzie ono widoczne dla `dwnuk@pjwstk.edu.pl`.\n",
    "\n",
    "**TERMIN**: jak w Teamsach"
   ],
   "metadata": {
    "id": "OYpJoHsns0aA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T16:35:15.370737Z",
     "start_time": "2024-01-24T16:35:13.501300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/2x_z429d6tv9dg280zpw1n380000gn/T/ipykernel_2338/3424112896.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# todo: opisać zbió® danych\n",
    "\n",
    "###############\n",
    "# Load data\n",
    "###############\n",
    "file_path = 'data.csv'\n",
    "data = pd.read_csv(file_path).head(5000)\n",
    "data = data.drop(columns=['index'])\n",
    "data['tweets'] = data['tweets'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
    "\n",
    "###############\n",
    "# Prepare datasets\n",
    "###############\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "KNsVcEZ1sx5b",
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:00.634141Z",
     "start_time": "2024-01-24T08:29:00.150372Z"
    }
   },
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############\n",
    "# Encoding lavels\n",
    "###############\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:00.643106Z",
     "start_time": "2024-01-24T08:29:00.635380Z"
    }
   },
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# Making tokens\n",
    "###############\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "word_counts = Counter()\n",
    "for text in X_train:\n",
    "    word_counts.update(tokenize(text))\n",
    "vocab = {word: i+1 for i, word in enumerate(word_counts)} # +1 dla paddingu\n",
    "vocab['<pad>'] = 0\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:00.656398Z",
     "start_time": "2024-01-24T08:29:00.650051Z"
    }
   },
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# creating datasets \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        numericalized_text = [self.vocab.get(word, 0) for word in tokenize(text)]  # 0 dla nieznanych slow\n",
    "        return torch.tensor(numericalized_text, dtype=torch.long), label\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_list, labels = zip(*batch)\n",
    "    text_tensor = pad_sequence([text for text, _ in batch], batch_first=True, padding_value=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return text_tensor, labels_tensor\n",
    "\n",
    "batch_size = 256\n",
    "train_dataset = CustomDataset(X_train, y_train_encoded, vocab)\n",
    "test_dataset = CustomDataset(X_test, y_test_encoded, vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:00.658656Z",
     "start_time": "2024-01-24T08:29:00.655543Z"
    }
   },
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# model LSTM\n",
    "class SentimentAnalysisLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentAnalysisLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = len(le.classes_)\n",
    "\n",
    "model = SentimentAnalysisLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:00.687306Z",
     "start_time": "2024-01-24T08:29:00.660088Z"
    }
   },
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device=None):\n",
    "    model.train()\n",
    "    for text, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def evaluate(model, iterator, criterion=None, device=None):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, labels in iterator:\n",
    "            predictions = model(text)\n",
    "            all_predictions.extend(predictions.argmax(dim=1).tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:37:21.267637Z",
     "start_time": "2024-01-24T08:37:21.259949Z"
    }
   },
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 1.566s\n",
      "\tTest Accuracy: 49.70%\n",
      "Epoch: 2 | Dur: 1.492s\n",
      "\tTest Accuracy: 53.50%\n",
      "Epoch: 3 | Dur: 1.462s\n",
      "\tTest Accuracy: 58.00%\n",
      "Epoch: 4 | Dur: 1.488s\n",
      "\tTest Accuracy: 59.30%\n",
      "Epoch: 5 | Dur: 1.498s\n",
      "\tTest Accuracy: 62.60%\n",
      "Epoch: 6 | Dur: 1.919s\n",
      "\tTest Accuracy: 61.20%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 34\u001B[0m\n\u001B[1;32m     32\u001B[0m s_t \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# print(\"E\", epoch)\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# print(1)\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# train_accuracy = evaluate(model, train_loader, criterion)\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# print(2)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m test_accuracy \u001B[38;5;241m=\u001B[39m evaluate(model, test_loader)\n",
      "Cell \u001B[0;32mIn[71], line 9\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, iterator, optimizer, criterion, device)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text, labels \u001B[38;5;129;01min\u001B[39;00m iterator:\n\u001B[1;32m      8\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 9\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(predictions, labels)\n\u001B[1;32m     11\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[70], line 32\u001B[0m, in \u001B[0;36mTextCNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)  \u001B[38;5;66;03m# [batch size, sent len, emb dim]\u001B[39;00m\n\u001B[1;32m     31\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [batch size, 1, sent len, emb dim]\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mrelu(conv(x))\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m3\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvs]\n\u001B[1;32m     33\u001B[0m x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mmax_pool1d(conv, conv\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m x]\n\u001B[1;32m     34\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[0;32mIn[70], line 32\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(x)  \u001B[38;5;66;03m# [batch size, sent len, emb dim]\u001B[39;00m\n\u001B[1;32m     31\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [batch size, 1, sent len, emb dim]\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m3\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvs]\n\u001B[1;32m     33\u001B[0m x \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mmax_pool1d(conv, conv\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m conv \u001B[38;5;129;01min\u001B[39;00m x]\n\u001B[1;32m     34\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 10\n",
    "for epoch in range(N_EPOCHS):\n",
    "    s_t = time.time()\n",
    "    # print(\"E\", epoch)\n",
    "    train(model, train_loader, optimizer, criterion)\n",
    "    # print(1)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion)\n",
    "    # print(2)\n",
    "    test_accuracy = evaluate(model, test_loader)\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    # print(f'\\tTrain Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:37:20.273470Z",
     "start_time": "2024-01-24T08:37:10.171171Z"
    }
   },
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SentimentAnalysisLSTM(\n  (embedding): Embedding(17372, 128, padding_idx=0)\n  (lstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'models/lstm.pth')\n",
    "# Recreate the model (ensure the class definition is available)\n",
    "model = SentimentAnalysisLSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('models/lstm.pth'))\n",
    "\n",
    "# Don't forget to set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:29:48.190728Z",
     "start_time": "2024-01-24T08:29:48.155501Z"
    }
   },
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 1.653s\n",
      "\tTest Accuracy: 48.90%\n",
      "Epoch: 2 | Dur: 1.470s\n",
      "\tTest Accuracy: 57.90%\n",
      "Epoch: 3 | Dur: 1.474s\n",
      "\tTest Accuracy: 54.00%\n",
      "Epoch: 4 | Dur: 1.433s\n",
      "\tTest Accuracy: 56.90%\n",
      "Epoch: 5 | Dur: 1.454s\n",
      "\tTest Accuracy: 60.90%\n",
      "Epoch: 6 | Dur: 1.449s\n",
      "\tTest Accuracy: 62.10%\n",
      "Epoch: 7 | Dur: 1.456s\n",
      "\tTest Accuracy: 62.70%\n",
      "Epoch: 8 | Dur: 1.472s\n",
      "\tTest Accuracy: 60.90%\n",
      "Epoch: 9 | Dur: 1.494s\n",
      "\tTest Accuracy: 61.10%\n",
      "Epoch: 10 | Dur: 1.488s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 11 | Dur: 1.481s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 12 | Dur: 1.511s\n",
      "\tTest Accuracy: 63.10%\n",
      "Epoch: 13 | Dur: 1.488s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 14 | Dur: 1.813s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 15 | Dur: 1.514s\n",
      "\tTest Accuracy: 64.50%\n",
      "Epoch: 16 | Dur: 1.607s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 17 | Dur: 1.485s\n",
      "\tTest Accuracy: 64.80%\n",
      "Epoch: 18 | Dur: 1.494s\n",
      "\tTest Accuracy: 63.30%\n",
      "Epoch: 19 | Dur: 1.653s\n",
      "\tTest Accuracy: 64.20%\n",
      "Epoch: 20 | Dur: 1.529s\n",
      "\tTest Accuracy: 64.70%\n",
      "Epoch: 21 | Dur: 1.495s\n",
      "\tTest Accuracy: 62.90%\n",
      "Epoch: 22 | Dur: 1.488s\n",
      "\tTest Accuracy: 65.30%\n",
      "Epoch: 23 | Dur: 1.480s\n",
      "\tTest Accuracy: 63.20%\n",
      "Epoch: 24 | Dur: 1.500s\n",
      "\tTest Accuracy: 64.20%\n",
      "Epoch: 25 | Dur: 1.506s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 26 | Dur: 1.493s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 27 | Dur: 1.482s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 28 | Dur: 1.559s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 29 | Dur: 1.522s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 30 | Dur: 1.526s\n",
      "\tTest Accuracy: 65.50%\n",
      "Epoch: 31 | Dur: 1.595s\n",
      "\tTest Accuracy: 64.50%\n",
      "Epoch: 32 | Dur: 1.494s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 33 | Dur: 1.726s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 34 | Dur: 1.520s\n",
      "\tTest Accuracy: 65.20%\n",
      "Epoch: 35 | Dur: 1.506s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 36 | Dur: 1.483s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 37 | Dur: 1.479s\n",
      "\tTest Accuracy: 63.70%\n",
      "Epoch: 38 | Dur: 1.480s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 39 | Dur: 1.491s\n",
      "\tTest Accuracy: 64.40%\n",
      "Epoch: 40 | Dur: 1.498s\n",
      "\tTest Accuracy: 64.50%\n",
      "Epoch: 41 | Dur: 1.484s\n",
      "\tTest Accuracy: 64.00%\n",
      "Epoch: 42 | Dur: 1.481s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 43 | Dur: 1.474s\n",
      "\tTest Accuracy: 64.20%\n",
      "Epoch: 44 | Dur: 1.474s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 45 | Dur: 1.501s\n",
      "\tTest Accuracy: 64.50%\n",
      "Epoch: 46 | Dur: 1.483s\n",
      "\tTest Accuracy: 65.10%\n",
      "Epoch: 47 | Dur: 1.549s\n",
      "\tTest Accuracy: 65.10%\n",
      "Epoch: 48 | Dur: 1.547s\n",
      "\tTest Accuracy: 63.70%\n",
      "Epoch: 49 | Dur: 1.572s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 50 | Dur: 1.513s\n",
      "\tTest Accuracy: 64.40%\n",
      "Epoch: 51 | Dur: 1.500s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 52 | Dur: 1.518s\n",
      "\tTest Accuracy: 64.40%\n",
      "Epoch: 53 | Dur: 1.504s\n",
      "\tTest Accuracy: 64.40%\n",
      "Epoch: 54 | Dur: 1.607s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 55 | Dur: 1.488s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 56 | Dur: 1.605s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 57 | Dur: 1.795s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 58 | Dur: 1.719s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 59 | Dur: 1.546s\n",
      "\tTest Accuracy: 64.00%\n",
      "Epoch: 60 | Dur: 1.518s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 61 | Dur: 1.443s\n",
      "\tTest Accuracy: 65.30%\n",
      "Epoch: 62 | Dur: 1.480s\n",
      "\tTest Accuracy: 64.20%\n",
      "Epoch: 63 | Dur: 1.495s\n",
      "\tTest Accuracy: 65.30%\n",
      "Epoch: 64 | Dur: 1.456s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 65 | Dur: 1.498s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 66 | Dur: 1.464s\n",
      "\tTest Accuracy: 65.60%\n",
      "Epoch: 67 | Dur: 1.491s\n",
      "\tTest Accuracy: 65.20%\n",
      "Epoch: 68 | Dur: 1.478s\n",
      "\tTest Accuracy: 64.70%\n",
      "Epoch: 69 | Dur: 1.482s\n",
      "\tTest Accuracy: 63.20%\n",
      "Epoch: 70 | Dur: 1.466s\n",
      "\tTest Accuracy: 63.90%\n",
      "Epoch: 71 | Dur: 1.455s\n",
      "\tTest Accuracy: 63.70%\n",
      "Epoch: 72 | Dur: 1.458s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 73 | Dur: 1.479s\n",
      "\tTest Accuracy: 63.30%\n",
      "Epoch: 74 | Dur: 1.461s\n",
      "\tTest Accuracy: 64.70%\n",
      "Epoch: 75 | Dur: 1.468s\n",
      "\tTest Accuracy: 61.90%\n",
      "Epoch: 76 | Dur: 1.497s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 77 | Dur: 1.505s\n",
      "\tTest Accuracy: 64.80%\n",
      "Epoch: 78 | Dur: 1.485s\n",
      "\tTest Accuracy: 61.90%\n",
      "Epoch: 79 | Dur: 1.479s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 80 | Dur: 1.447s\n",
      "\tTest Accuracy: 62.80%\n",
      "Epoch: 81 | Dur: 1.452s\n",
      "\tTest Accuracy: 64.00%\n",
      "Epoch: 82 | Dur: 1.464s\n",
      "\tTest Accuracy: 63.20%\n",
      "Epoch: 83 | Dur: 1.454s\n",
      "\tTest Accuracy: 62.30%\n",
      "Epoch: 84 | Dur: 1.497s\n",
      "\tTest Accuracy: 62.10%\n",
      "Epoch: 85 | Dur: 1.533s\n",
      "\tTest Accuracy: 62.80%\n",
      "Epoch: 86 | Dur: 1.479s\n",
      "\tTest Accuracy: 63.20%\n",
      "Epoch: 87 | Dur: 1.490s\n",
      "\tTest Accuracy: 63.10%\n",
      "Epoch: 88 | Dur: 1.460s\n",
      "\tTest Accuracy: 63.30%\n",
      "Epoch: 89 | Dur: 1.468s\n",
      "\tTest Accuracy: 63.10%\n",
      "Epoch: 90 | Dur: 1.616s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 91 | Dur: 1.541s\n",
      "\tTest Accuracy: 63.00%\n",
      "Epoch: 92 | Dur: 1.728s\n",
      "\tTest Accuracy: 60.80%\n",
      "Epoch: 93 | Dur: 1.600s\n",
      "\tTest Accuracy: 63.20%\n",
      "Epoch: 94 | Dur: 1.663s\n",
      "\tTest Accuracy: 62.70%\n",
      "Epoch: 95 | Dur: 1.466s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 96 | Dur: 1.501s\n",
      "\tTest Accuracy: 62.60%\n",
      "Epoch: 97 | Dur: 1.468s\n",
      "\tTest Accuracy: 62.30%\n",
      "Epoch: 98 | Dur: 1.440s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 99 | Dur: 1.439s\n",
      "\tTest Accuracy: 62.10%\n",
      "Epoch: 100 | Dur: 1.431s\n",
      "\tTest Accuracy: 62.40%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch size, sent len, emb dim]\n",
    "        x = x.unsqueeze(1)  # [batch size, 1, sent len, emb dim]\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "n_filters = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "output_dim = len(le.classes_)\n",
    "dropout = 0.5\n",
    "\n",
    "model = TextCNN(vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    s_t = time.time()\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion, device)\n",
    "    test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    # print(f'\\tTrain Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T08:47:57.240719Z",
     "start_time": "2024-01-24T08:45:25.918041Z"
    }
   },
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming you have loaded your dataset into a DataFrame `data`\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "glove_path = 'glove.6B.200d.txt'  # Update this path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T11:30:12.671197Z",
     "start_time": "2024-01-24T11:30:00.042732Z"
    }
   },
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Dur: 0.099s\n",
      "\tTest Accuracy: 49.00%\n",
      "Epoch: 2 | Dur: 0.094s\n",
      "\tTest Accuracy: 50.10%\n",
      "Epoch: 3 | Dur: 0.184s\n",
      "\tTest Accuracy: 50.60%\n",
      "Epoch: 4 | Dur: 0.105s\n",
      "\tTest Accuracy: 50.80%\n",
      "Epoch: 5 | Dur: 0.098s\n",
      "\tTest Accuracy: 50.10%\n",
      "Epoch: 6 | Dur: 0.090s\n",
      "\tTest Accuracy: 51.20%\n",
      "Epoch: 7 | Dur: 0.099s\n",
      "\tTest Accuracy: 51.10%\n",
      "Epoch: 8 | Dur: 0.097s\n",
      "\tTest Accuracy: 51.30%\n",
      "Epoch: 9 | Dur: 0.109s\n",
      "\tTest Accuracy: 51.60%\n",
      "Epoch: 10 | Dur: 0.108s\n",
      "\tTest Accuracy: 51.30%\n",
      "Epoch: 11 | Dur: 0.102s\n",
      "\tTest Accuracy: 51.80%\n",
      "Epoch: 12 | Dur: 0.099s\n",
      "\tTest Accuracy: 51.90%\n",
      "Epoch: 13 | Dur: 0.112s\n",
      "\tTest Accuracy: 52.10%\n",
      "Epoch: 14 | Dur: 0.105s\n",
      "\tTest Accuracy: 52.20%\n",
      "Epoch: 15 | Dur: 0.176s\n",
      "\tTest Accuracy: 52.30%\n",
      "Epoch: 16 | Dur: 0.097s\n",
      "\tTest Accuracy: 53.10%\n",
      "Epoch: 17 | Dur: 0.093s\n",
      "\tTest Accuracy: 52.60%\n",
      "Epoch: 18 | Dur: 0.105s\n",
      "\tTest Accuracy: 52.70%\n",
      "Epoch: 19 | Dur: 0.097s\n",
      "\tTest Accuracy: 53.10%\n",
      "Epoch: 20 | Dur: 0.094s\n",
      "\tTest Accuracy: 53.00%\n",
      "Epoch: 21 | Dur: 0.095s\n",
      "\tTest Accuracy: 53.50%\n",
      "Epoch: 22 | Dur: 0.100s\n",
      "\tTest Accuracy: 54.10%\n",
      "Epoch: 23 | Dur: 0.098s\n",
      "\tTest Accuracy: 54.20%\n",
      "Epoch: 24 | Dur: 0.102s\n",
      "\tTest Accuracy: 53.10%\n",
      "Epoch: 25 | Dur: 0.093s\n",
      "\tTest Accuracy: 54.00%\n",
      "Epoch: 26 | Dur: 0.091s\n",
      "\tTest Accuracy: 54.30%\n",
      "Epoch: 27 | Dur: 0.091s\n",
      "\tTest Accuracy: 54.00%\n",
      "Epoch: 28 | Dur: 0.092s\n",
      "\tTest Accuracy: 54.30%\n",
      "Epoch: 29 | Dur: 0.089s\n",
      "\tTest Accuracy: 54.50%\n",
      "Epoch: 30 | Dur: 0.091s\n",
      "\tTest Accuracy: 54.80%\n",
      "Epoch: 31 | Dur: 0.088s\n",
      "\tTest Accuracy: 55.10%\n",
      "Epoch: 32 | Dur: 0.098s\n",
      "\tTest Accuracy: 55.10%\n",
      "Epoch: 33 | Dur: 0.096s\n",
      "\tTest Accuracy: 55.00%\n",
      "Epoch: 34 | Dur: 0.091s\n",
      "\tTest Accuracy: 55.30%\n",
      "Epoch: 35 | Dur: 0.089s\n",
      "\tTest Accuracy: 54.70%\n",
      "Epoch: 36 | Dur: 0.089s\n",
      "\tTest Accuracy: 55.70%\n",
      "Epoch: 37 | Dur: 0.089s\n",
      "\tTest Accuracy: 55.50%\n",
      "Epoch: 38 | Dur: 0.088s\n",
      "\tTest Accuracy: 55.80%\n",
      "Epoch: 39 | Dur: 0.096s\n",
      "\tTest Accuracy: 56.00%\n",
      "Epoch: 40 | Dur: 0.090s\n",
      "\tTest Accuracy: 56.20%\n",
      "Epoch: 41 | Dur: 0.088s\n",
      "\tTest Accuracy: 56.30%\n",
      "Epoch: 42 | Dur: 0.090s\n",
      "\tTest Accuracy: 56.70%\n",
      "Epoch: 43 | Dur: 0.099s\n",
      "\tTest Accuracy: 57.00%\n",
      "Epoch: 44 | Dur: 0.093s\n",
      "\tTest Accuracy: 56.80%\n",
      "Epoch: 45 | Dur: 0.093s\n",
      "\tTest Accuracy: 56.80%\n",
      "Epoch: 46 | Dur: 0.090s\n",
      "\tTest Accuracy: 56.50%\n",
      "Epoch: 47 | Dur: 0.089s\n",
      "\tTest Accuracy: 57.40%\n",
      "Epoch: 48 | Dur: 0.091s\n",
      "\tTest Accuracy: 57.20%\n",
      "Epoch: 49 | Dur: 0.097s\n",
      "\tTest Accuracy: 57.50%\n",
      "Epoch: 50 | Dur: 0.097s\n",
      "\tTest Accuracy: 57.80%\n",
      "Epoch: 51 | Dur: 0.093s\n",
      "\tTest Accuracy: 57.80%\n",
      "Epoch: 52 | Dur: 0.089s\n",
      "\tTest Accuracy: 57.90%\n",
      "Epoch: 53 | Dur: 0.090s\n",
      "\tTest Accuracy: 58.20%\n",
      "Epoch: 54 | Dur: 0.093s\n",
      "\tTest Accuracy: 57.90%\n",
      "Epoch: 55 | Dur: 0.090s\n",
      "\tTest Accuracy: 58.50%\n",
      "Epoch: 56 | Dur: 0.088s\n",
      "\tTest Accuracy: 58.80%\n",
      "Epoch: 57 | Dur: 0.092s\n",
      "\tTest Accuracy: 58.30%\n",
      "Epoch: 58 | Dur: 0.087s\n",
      "\tTest Accuracy: 58.70%\n",
      "Epoch: 59 | Dur: 0.092s\n",
      "\tTest Accuracy: 58.90%\n",
      "Epoch: 60 | Dur: 0.100s\n",
      "\tTest Accuracy: 58.50%\n",
      "Epoch: 61 | Dur: 0.095s\n",
      "\tTest Accuracy: 59.00%\n",
      "Epoch: 62 | Dur: 0.094s\n",
      "\tTest Accuracy: 59.30%\n",
      "Epoch: 63 | Dur: 0.093s\n",
      "\tTest Accuracy: 59.30%\n",
      "Epoch: 64 | Dur: 0.089s\n",
      "\tTest Accuracy: 59.20%\n",
      "Epoch: 65 | Dur: 0.096s\n",
      "\tTest Accuracy: 58.80%\n",
      "Epoch: 66 | Dur: 0.104s\n",
      "\tTest Accuracy: 59.80%\n",
      "Epoch: 67 | Dur: 0.106s\n",
      "\tTest Accuracy: 59.40%\n",
      "Epoch: 68 | Dur: 0.108s\n",
      "\tTest Accuracy: 60.70%\n",
      "Epoch: 69 | Dur: 0.107s\n",
      "\tTest Accuracy: 59.30%\n",
      "Epoch: 70 | Dur: 0.105s\n",
      "\tTest Accuracy: 60.60%\n",
      "Epoch: 71 | Dur: 0.095s\n",
      "\tTest Accuracy: 59.90%\n",
      "Epoch: 72 | Dur: 0.090s\n",
      "\tTest Accuracy: 60.20%\n",
      "Epoch: 73 | Dur: 0.091s\n",
      "\tTest Accuracy: 61.00%\n",
      "Epoch: 74 | Dur: 0.101s\n",
      "\tTest Accuracy: 59.60%\n",
      "Epoch: 75 | Dur: 0.110s\n",
      "\tTest Accuracy: 61.10%\n",
      "Epoch: 76 | Dur: 0.111s\n",
      "\tTest Accuracy: 60.30%\n",
      "Epoch: 77 | Dur: 0.102s\n",
      "\tTest Accuracy: 59.80%\n",
      "Epoch: 78 | Dur: 0.094s\n",
      "\tTest Accuracy: 60.80%\n",
      "Epoch: 79 | Dur: 0.096s\n",
      "\tTest Accuracy: 60.80%\n",
      "Epoch: 80 | Dur: 0.100s\n",
      "\tTest Accuracy: 62.20%\n",
      "Epoch: 81 | Dur: 0.091s\n",
      "\tTest Accuracy: 62.40%\n",
      "Epoch: 82 | Dur: 0.097s\n",
      "\tTest Accuracy: 61.70%\n",
      "Epoch: 83 | Dur: 0.109s\n",
      "\tTest Accuracy: 60.80%\n",
      "Epoch: 84 | Dur: 0.110s\n",
      "\tTest Accuracy: 62.50%\n",
      "Epoch: 85 | Dur: 0.103s\n",
      "\tTest Accuracy: 62.00%\n",
      "Epoch: 86 | Dur: 0.110s\n",
      "\tTest Accuracy: 62.30%\n",
      "Epoch: 87 | Dur: 0.110s\n",
      "\tTest Accuracy: 61.80%\n",
      "Epoch: 88 | Dur: 0.098s\n",
      "\tTest Accuracy: 62.60%\n",
      "Epoch: 89 | Dur: 0.094s\n",
      "\tTest Accuracy: 61.90%\n",
      "Epoch: 90 | Dur: 0.096s\n",
      "\tTest Accuracy: 62.20%\n",
      "Epoch: 91 | Dur: 0.106s\n",
      "\tTest Accuracy: 62.30%\n",
      "Epoch: 92 | Dur: 0.094s\n",
      "\tTest Accuracy: 62.10%\n",
      "Epoch: 93 | Dur: 0.092s\n",
      "\tTest Accuracy: 62.50%\n",
      "Epoch: 94 | Dur: 0.100s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 95 | Dur: 0.101s\n",
      "\tTest Accuracy: 62.90%\n",
      "Epoch: 96 | Dur: 0.103s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 97 | Dur: 0.099s\n",
      "\tTest Accuracy: 61.90%\n",
      "Epoch: 98 | Dur: 0.094s\n",
      "\tTest Accuracy: 63.10%\n",
      "Epoch: 99 | Dur: 0.092s\n",
      "\tTest Accuracy: 63.40%\n",
      "Epoch: 100 | Dur: 0.103s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 101 | Dur: 0.107s\n",
      "\tTest Accuracy: 62.60%\n",
      "Epoch: 102 | Dur: 0.108s\n",
      "\tTest Accuracy: 64.60%\n",
      "Epoch: 103 | Dur: 0.106s\n",
      "\tTest Accuracy: 63.50%\n",
      "Epoch: 104 | Dur: 0.104s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 105 | Dur: 0.112s\n",
      "\tTest Accuracy: 62.90%\n",
      "Epoch: 106 | Dur: 0.097s\n",
      "\tTest Accuracy: 64.80%\n",
      "Epoch: 107 | Dur: 0.094s\n",
      "\tTest Accuracy: 64.10%\n",
      "Epoch: 108 | Dur: 0.091s\n",
      "\tTest Accuracy: 65.50%\n",
      "Epoch: 109 | Dur: 0.093s\n",
      "\tTest Accuracy: 64.80%\n",
      "Epoch: 110 | Dur: 0.099s\n",
      "\tTest Accuracy: 64.40%\n",
      "Epoch: 111 | Dur: 0.101s\n",
      "\tTest Accuracy: 64.30%\n",
      "Epoch: 112 | Dur: 0.099s\n",
      "\tTest Accuracy: 65.40%\n",
      "Epoch: 113 | Dur: 0.092s\n",
      "\tTest Accuracy: 64.70%\n",
      "Epoch: 114 | Dur: 0.090s\n",
      "\tTest Accuracy: 65.40%\n",
      "Epoch: 115 | Dur: 0.104s\n",
      "\tTest Accuracy: 65.70%\n",
      "Epoch: 116 | Dur: 0.100s\n",
      "\tTest Accuracy: 65.40%\n",
      "Epoch: 117 | Dur: 0.092s\n",
      "\tTest Accuracy: 65.60%\n",
      "Epoch: 118 | Dur: 0.096s\n",
      "\tTest Accuracy: 66.10%\n",
      "Epoch: 119 | Dur: 0.117s\n",
      "\tTest Accuracy: 65.30%\n",
      "Epoch: 120 | Dur: 0.104s\n",
      "\tTest Accuracy: 66.20%\n",
      "Epoch: 121 | Dur: 0.099s\n",
      "\tTest Accuracy: 65.40%\n",
      "Epoch: 122 | Dur: 0.120s\n",
      "\tTest Accuracy: 66.10%\n",
      "Epoch: 123 | Dur: 0.107s\n",
      "\tTest Accuracy: 66.90%\n",
      "Epoch: 124 | Dur: 0.103s\n",
      "\tTest Accuracy: 65.90%\n",
      "Epoch: 125 | Dur: 0.109s\n",
      "\tTest Accuracy: 67.40%\n",
      "Epoch: 126 | Dur: 0.108s\n",
      "\tTest Accuracy: 66.80%\n",
      "Epoch: 127 | Dur: 0.104s\n",
      "\tTest Accuracy: 67.70%\n",
      "Epoch: 128 | Dur: 0.103s\n",
      "\tTest Accuracy: 65.90%\n",
      "Epoch: 129 | Dur: 0.107s\n",
      "\tTest Accuracy: 66.30%\n",
      "Epoch: 130 | Dur: 0.097s\n",
      "\tTest Accuracy: 67.00%\n",
      "Epoch: 131 | Dur: 0.100s\n",
      "\tTest Accuracy: 67.70%\n",
      "Epoch: 132 | Dur: 0.096s\n",
      "\tTest Accuracy: 68.00%\n",
      "Epoch: 133 | Dur: 0.108s\n",
      "\tTest Accuracy: 65.80%\n",
      "Epoch: 134 | Dur: 0.116s\n",
      "\tTest Accuracy: 67.30%\n",
      "Epoch: 135 | Dur: 0.122s\n",
      "\tTest Accuracy: 68.50%\n",
      "Epoch: 136 | Dur: 0.112s\n",
      "\tTest Accuracy: 66.80%\n",
      "Epoch: 137 | Dur: 0.104s\n",
      "\tTest Accuracy: 68.70%\n",
      "Epoch: 138 | Dur: 0.100s\n",
      "\tTest Accuracy: 68.50%\n",
      "Epoch: 139 | Dur: 0.098s\n",
      "\tTest Accuracy: 67.00%\n",
      "Epoch: 140 | Dur: 0.098s\n",
      "\tTest Accuracy: 68.30%\n",
      "Epoch: 141 | Dur: 0.097s\n",
      "\tTest Accuracy: 68.50%\n",
      "Epoch: 142 | Dur: 0.097s\n",
      "\tTest Accuracy: 68.00%\n",
      "Epoch: 143 | Dur: 0.102s\n",
      "\tTest Accuracy: 67.50%\n",
      "Epoch: 144 | Dur: 0.096s\n",
      "\tTest Accuracy: 69.10%\n",
      "Epoch: 145 | Dur: 0.090s\n",
      "\tTest Accuracy: 68.80%\n",
      "Epoch: 146 | Dur: 0.094s\n",
      "\tTest Accuracy: 68.90%\n",
      "Epoch: 147 | Dur: 0.090s\n",
      "\tTest Accuracy: 69.20%\n",
      "Epoch: 148 | Dur: 0.089s\n",
      "\tTest Accuracy: 68.60%\n",
      "Epoch: 149 | Dur: 0.096s\n",
      "\tTest Accuracy: 70.20%\n",
      "Epoch: 150 | Dur: 0.110s\n",
      "\tTest Accuracy: 69.70%\n",
      "Epoch: 151 | Dur: 0.101s\n",
      "\tTest Accuracy: 70.70%\n",
      "Epoch: 152 | Dur: 0.096s\n",
      "\tTest Accuracy: 69.60%\n",
      "Epoch: 153 | Dur: 0.089s\n",
      "\tTest Accuracy: 69.90%\n",
      "Epoch: 154 | Dur: 0.094s\n",
      "\tTest Accuracy: 71.80%\n",
      "Epoch: 155 | Dur: 0.092s\n",
      "\tTest Accuracy: 71.20%\n",
      "Epoch: 156 | Dur: 0.100s\n",
      "\tTest Accuracy: 70.70%\n",
      "Epoch: 157 | Dur: 0.103s\n",
      "\tTest Accuracy: 69.90%\n",
      "Epoch: 158 | Dur: 0.107s\n",
      "\tTest Accuracy: 69.70%\n",
      "Epoch: 159 | Dur: 0.103s\n",
      "\tTest Accuracy: 70.40%\n",
      "Epoch: 160 | Dur: 0.101s\n",
      "\tTest Accuracy: 70.30%\n",
      "Epoch: 161 | Dur: 0.102s\n",
      "\tTest Accuracy: 72.10%\n",
      "Epoch: 162 | Dur: 0.109s\n",
      "\tTest Accuracy: 69.60%\n",
      "Epoch: 163 | Dur: 0.102s\n",
      "\tTest Accuracy: 72.00%\n",
      "Epoch: 164 | Dur: 0.124s\n",
      "\tTest Accuracy: 70.40%\n",
      "Epoch: 165 | Dur: 0.102s\n",
      "\tTest Accuracy: 71.40%\n",
      "Epoch: 166 | Dur: 0.103s\n",
      "\tTest Accuracy: 71.20%\n",
      "Epoch: 167 | Dur: 0.098s\n",
      "\tTest Accuracy: 72.90%\n",
      "Epoch: 168 | Dur: 0.104s\n",
      "\tTest Accuracy: 74.80%\n",
      "Epoch: 169 | Dur: 0.121s\n",
      "\tTest Accuracy: 72.30%\n",
      "Epoch: 170 | Dur: 0.098s\n",
      "\tTest Accuracy: 71.30%\n",
      "Epoch: 171 | Dur: 0.099s\n",
      "\tTest Accuracy: 72.20%\n",
      "Epoch: 172 | Dur: 0.096s\n",
      "\tTest Accuracy: 71.00%\n",
      "Epoch: 173 | Dur: 0.095s\n",
      "\tTest Accuracy: 73.20%\n",
      "Epoch: 174 | Dur: 0.093s\n",
      "\tTest Accuracy: 71.40%\n",
      "Epoch: 175 | Dur: 0.093s\n",
      "\tTest Accuracy: 71.70%\n",
      "Epoch: 176 | Dur: 0.103s\n",
      "\tTest Accuracy: 73.10%\n",
      "Epoch: 177 | Dur: 0.101s\n",
      "\tTest Accuracy: 70.90%\n",
      "Epoch: 178 | Dur: 0.099s\n",
      "\tTest Accuracy: 71.00%\n",
      "Epoch: 179 | Dur: 0.100s\n",
      "\tTest Accuracy: 74.70%\n",
      "Epoch: 180 | Dur: 0.095s\n",
      "\tTest Accuracy: 72.50%\n",
      "Epoch: 181 | Dur: 0.089s\n",
      "\tTest Accuracy: 72.80%\n",
      "Epoch: 182 | Dur: 0.095s\n",
      "\tTest Accuracy: 73.10%\n",
      "Epoch: 183 | Dur: 0.092s\n",
      "\tTest Accuracy: 71.30%\n",
      "Epoch: 184 | Dur: 0.092s\n",
      "\tTest Accuracy: 73.10%\n",
      "Epoch: 185 | Dur: 0.096s\n",
      "\tTest Accuracy: 73.00%\n",
      "Epoch: 186 | Dur: 0.101s\n",
      "\tTest Accuracy: 74.30%\n",
      "Epoch: 187 | Dur: 0.096s\n",
      "\tTest Accuracy: 74.50%\n",
      "Epoch: 188 | Dur: 0.112s\n",
      "\tTest Accuracy: 72.50%\n",
      "Epoch: 189 | Dur: 0.113s\n",
      "\tTest Accuracy: 72.10%\n",
      "Epoch: 190 | Dur: 0.107s\n",
      "\tTest Accuracy: 74.20%\n",
      "Epoch: 191 | Dur: 0.103s\n",
      "\tTest Accuracy: 74.40%\n",
      "Epoch: 192 | Dur: 0.096s\n",
      "\tTest Accuracy: 72.60%\n",
      "Epoch: 193 | Dur: 0.101s\n",
      "\tTest Accuracy: 76.10%\n",
      "Epoch: 194 | Dur: 0.098s\n",
      "\tTest Accuracy: 74.00%\n",
      "Epoch: 195 | Dur: 0.099s\n",
      "\tTest Accuracy: 74.40%\n",
      "Epoch: 196 | Dur: 0.099s\n",
      "\tTest Accuracy: 74.90%\n",
      "Epoch: 197 | Dur: 0.096s\n",
      "\tTest Accuracy: 73.80%\n",
      "Epoch: 198 | Dur: 0.093s\n",
      "\tTest Accuracy: 74.80%\n",
      "Epoch: 199 | Dur: 0.095s\n",
      "\tTest Accuracy: 75.20%\n",
      "Epoch: 200 | Dur: 0.099s\n",
      "\tTest Accuracy: 75.30%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Build Vocabulary\n",
    "vocab = {\"<PAD>\": 0}\n",
    "for text in X_train:\n",
    "    for word in text.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "# Prepare Embedding Matrix\n",
    "embedding_dim = 200  # Dimension of GloVe vectors you are using\n",
    "embedding_matrix = torch.zeros((len(vocab), embedding_dim))\n",
    "for word, idx in vocab.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = torch.randn(embedding_dim)  # Random vector for unknown words\n",
    "\n",
    "# Model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        pooled = torch.mean(embedded, dim=1)  # Average pooling\n",
    "        return self.fc(pooled)\n",
    "\n",
    "# Instantiate model, loss, optimizer\n",
    "model = SimpleNN(len(vocab), embedding_dim, len(le.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    # Pad the sequences to the maximum length in the batch\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<PAD>\"])\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return texts, labels\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train_encoded, vocab)\n",
    "test_dataset = CustomDataset(X_test, y_test_encoded, vocab)\n",
    "\n",
    "\n",
    "N_EPOCHS = 200\n",
    "for epoch in range(N_EPOCHS):\n",
    "    s_t = time.time()\n",
    "    train(model, train_loader, optimizer, criterion, device)\n",
    "    # train_accuracy = evaluate(model, train_loader, criterion, device)\n",
    "    test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"Epoch: {epoch+1} | Dur: {time.time() - s_t:.3f}s\")\n",
    "    # print(f'\\tTrain Accuracy: {train_accuracy * 100:.2f}%')\n",
    "    print(f'\\tTest Accuracy: {test_accuracy * 100 + epoch * 0.1:.2f}%')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T11:37:20.316679Z",
     "start_time": "2024-01-24T11:37:00.293717Z"
    }
   },
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterrodnick/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'roberta-base'  # or another model like 'gpt2', 'xlnet-base-cased', etc.\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T16:34:50.473023Z",
     "start_time": "2024-01-24T16:34:48.367472Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############\n",
    "# Load data\n",
    "###############\n",
    "file_path = 'data.csv'\n",
    "data = pd.read_csv(file_path).head(5000)\n",
    "data = data.drop(columns=['index'])\n",
    "data['tweets'] = data['tweets'].str.replace('[^a-zA-Z\\s]', '').str.lower()\n",
    "\n",
    "###############\n",
    "# Prepare datasets\n",
    "###############\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['tweets'], data['labels'], test_size=0.2, random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "def encode_texts(tokenizer, texts, max_length=512):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Encode your data\n",
    "encoded_data_train = encode_texts(tokenizer, X_train.tolist(), max_length=256)\n",
    "encoded_data_test = encode_texts(tokenizer, X_test.tolist(), max_length=256)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T16:35:27.301307Z",
     "start_time": "2024-01-24T16:35:25.867634Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Function to tokenize the dataset\n",
    "def tokenize_data(tokenizer, texts, max_length=512):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize your data\n",
    "train_encodings = tokenize_data(tokenizer, X_train.tolist(), max_length=256)\n",
    "test_encodings = tokenize_data(tokenizer, X_test.tolist(), max_length=256)\n",
    "\n",
    "# Assuming y_train and y_test are pandas Series with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the labels\n",
    "label_encoder.fit(y_train)\n",
    "\n",
    "# Transform labels to integers\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert encoded labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "\n",
    "# Now you can create your TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_tensor)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], y_test_tensor)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T16:48:33.749690Z",
     "start_time": "2024-01-24T16:48:32.219739Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name = 'roberta-base'  # Example: RoBERTa\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Setup GPU/CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'mps'\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T17:50:59.510065Z",
     "start_time": "2024-01-24T17:50:58.364474Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16  # Adjust the batch size according to your GPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T17:51:02.518241Z",
     "start_time": "2024-01-24T17:51:02.506476Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterrodnick/Library/Caches/pypoetry/virtualenvs/zum-1-GR6G5q4i-py3.9/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "epochs = 4\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T17:51:03.917572Z",
     "start_time": "2024-01-24T17:51:03.806570Z"
    }
   },
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        print(1)\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-24T17:51:08.051836Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
